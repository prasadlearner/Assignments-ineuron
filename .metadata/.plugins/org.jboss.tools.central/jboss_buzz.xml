<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>How to run a custom server task in Red Hat Data Grid</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/07/18/how-run-custom-server-task-red-hat-data-grid" /><author><name>Torbjorn Dahlen</name></author><id>d88d032d-2289-48ea-a518-eea26fcb73be</id><updated>2023-07-18T07:00:00Z</updated><published>2023-07-18T07:00:00Z</published><summary type="html">&lt;p&gt;Custom server tasks can be deployed to the &lt;a href="https://www.redhat.com/en/technologies/jboss-middleware/data-grid"&gt;Red Hat Data Grid&lt;/a&gt; server for remote execution from the command line interface (CLI) and Hot Rod or REST clients. Tasks can be implemented as custom Java classes or as scripts in languages such as JavaScript.&lt;/p&gt; &lt;p&gt;In this article, we will deploy a Java class that will evict and reload the cache in order to pick up modified entries in the original database table from which we loaded the cache. Data Grid will automatically load new entries added to the database table, however modified rows will require reloading using a server task, as shown in the following example.&lt;/p&gt; &lt;h2&gt;How to deploy PostgreSQL with a custom image&lt;/h2&gt; &lt;p&gt;SQL cache stores let you load Red Hat Data Grid caches from existing database tables. Data Grid offers two types of SQL cache stores: table and query. In the following example, Data Grid will load entries from a single database table. It is also possible to use SQL queries to load entries from single or multiple database tables.&lt;/p&gt; &lt;p&gt;For more details, refer to &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.4/html/configuring_data_grid_caches/persistence#sql-cache-store_persistence"&gt;SQL cache stores&lt;/a&gt;. All source code for this tutorial can be found on &lt;a href="https://github.com/torbjorndahlen/infinispan-evict-cache"&gt;GitHub&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;We will use PostgreSQL to contain the table that will be loaded by Data Grid. To deploy PostgreSQL in &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;, we will use a modified image that initializes the database with a table and a Data Grid user with granted privileges to read from the table.&lt;/p&gt; &lt;pre&gt; FROM registry.redhat.io/rhel8/postgresql-12 LABEL description="This is a custom PostgreSQL container image which loads the database schema definitions and the data into the model and inventory tables " COPY db/load_db.sh /opt/app-root/src/postgresql-start/ COPY db/rpi-store-ddl.sql /opt/app-root/src/postgresql-start/ COPY db/rpi-store-dml.sql /opt/app-root/src/postgresql-start/ COPY db/rpi-store-role.sql /opt/app-root/src/postgresql-start/ USER root RUN chmod 774 /opt/app-root/src/postgresql-start/*.sh USER 26&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;load_db.sh&lt;/code&gt; script populates the table used to load the Data Grid cache and creates the user &lt;code&gt;infinispan&lt;/code&gt; with privileges to &lt;code&gt;SELECT&lt;/code&gt; from the &lt;code&gt;model&lt;/code&gt; table:&lt;/p&gt; &lt;pre&gt; #!/bin/bash START_DIR="$APP_DATA/src/postgresql-start" run_sql_script () { SQL_FILE=$1 psql -U postgres \ --echo-all \ -f $SQL_FILE \ -d $POSTGRESQL_DATABASE } run_sql_script $START_DIR/rpi-store-ddl.sql run_sql_script $START_DIR/rpi-store-dml.sql run_sql_script $START_DIR/rpi-store-role.sql&lt;/pre&gt; &lt;p&gt;Note that the user needs to be &lt;code&gt;postgres&lt;/code&gt; to create a new user.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;rpi-store-ddl.sql&lt;/code&gt; script will run the following SQL commands to create the &lt;code&gt;model&lt;/code&gt; table which will be used to load keys and values into the Data Grid cache:&lt;/p&gt; &lt;pre&gt; drop table if exists model; create table model ( id integer primary key, name varchar(20), model varchar(20), soc varchar(20), memory_mb integer, ethernet boolean, release_year integer );&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;rpi-store-dml.sql&lt;/code&gt;script will create three rows in the &lt;code&gt;model&lt;/code&gt; table:&lt;/p&gt; &lt;pre&gt; insert into model (id, name, model, soc, memory_mb, ethernet, release_year) values (1, 'Raspberry Pi', 'B', 'BCM2835', 256, TRUE, 2012); insert into model (id, name, model, soc, memory_mb, ethernet, release_year) values (2, 'Raspberry Pi Zero', 'Zero', 'BCM2835', 512, FALSE, 2015); insert into model (id, name, model, soc, memory_mb, ethernet, release_year) values (3, 'Raspberry Pi Zero', '2W', 'BCM2835', 512, FALSE, 2021);&lt;/pre&gt; &lt;p&gt;Finally, the &lt;code&gt;rpi-store-role.sql&lt;/code&gt; script will create the user &lt;code&gt;infinispan&lt;/code&gt; and grant &lt;code&gt;SELECT&lt;/code&gt; privileges on the &lt;code&gt;model&lt;/code&gt; table. This user will be provided to the JDBC connector used by Data Grid.&lt;/p&gt; &lt;pre&gt; CREATE USER infinispan WITH PASSWORD 'secret'; GRANT SELECT ON model TO infinispan;&lt;/pre&gt; &lt;p&gt;We can now deploy PostgreSQL in OpenShift using the modified image:&lt;/p&gt; &lt;pre&gt; $ oc new-project infinispan-demo $ oc new-build \ &gt; https://github.com/torbjorndahlen/infinispan-evict-cache \ &gt; --strategy=docker \ &gt; --name='postgresql-12-custom' $ oc new-app \ &gt; -e POSTGRESQL_USER=db \ &gt; -e POSTGRESQL_PASSWORD=secret \ &gt; -e POSTGRESQL_DATABASE=rpi-store \ &gt; postgresql-12-custom&lt;/pre&gt; &lt;p&gt;After deployment is complete, we can verify that the user &lt;code&gt;infinispan&lt;/code&gt;, the DB &lt;code&gt;rpi-store&lt;/code&gt;, and the &lt;code&gt;model&lt;/code&gt; table were created as expected:&lt;/p&gt; &lt;pre&gt; $ oc get pods NAME READY STATUS RESTARTS AGE postgresql-12-custom-1-build 0/1 Completed 0 2m40s postgresql-12-custom-0 1/1 Running 0 38s $ oc exec postgresql-12-custom-0 -- psql -U infinispan -d rpi-store -c "select * from model;" id | name | model | soc | memory_mb | ethernet | release_year ----+-------------------+-------+---------+-----------+----------+-------------- 1 | Raspberry Pi | B | BCM2835 | 256 | t | 2012 2 | Raspberry Pi Zero | Zero | BCM2835 | 512 | f | 2015 3 | Raspberry Pi Zero | 2W | BCM2835 | 512 | f | 2021 (3 rows)&lt;/pre&gt; &lt;h2&gt;Server task implementation&lt;/h2&gt; &lt;p&gt;The &lt;code&gt;EvictReloadTask&lt;/code&gt; class implements the &lt;code&gt;org.infinispan.tasks.ServerTask&lt;/code&gt; interface where the &lt;code&gt;call()&lt;/code&gt; method is invoked by Data Grid when called from the Hot Rod client:&lt;/p&gt; &lt;pre&gt; @MetaInfServices(ServerTask.class) public class EvictReloadTask implements ServerTask, java.io.Serializable { private static final ThreadLocal taskContext = new ThreadLocal&lt;&gt;(); @Override public String call() throws Exception { TaskContext ctx = taskContext.get(); AdvancedCache&lt;?, ?&gt; cache = ctx.getCacheManager().getCache("rpi-store").getAdvancedCache(); cache.withFlags(Flag.SKIP_CACHE_STORE).clear(); cache.getComponentRegistry().getComponent(PreloadManager.class).start(); return null; } }&lt;/pre&gt; &lt;p&gt;Before deploying Data Grid, the server task is packaged in a JAR file containing the server task classes and a file in the &lt;code&gt;META-INF/services&lt;/code&gt; directory. This file is named &lt;code&gt;org.infinispan.tasks.ServerTask&lt;/code&gt; and contains the fully qualified name of the server task.&lt;/p&gt; &lt;pre&gt; example.EvictReloadTask&lt;/pre&gt; &lt;p&gt;You also need to add your server task classes to a deserialization allow list, since Data Grid does not allow deserialization of arbitrary Java classes for security reasons. To do this, we create a ConfigMap containing an allow-list for serializing of the server task class:&lt;/p&gt; &lt;pre&gt; apiVersion: v1 kind: ConfigMap metadata: name: cluster-config namespace: infinispan-demo data: infinispan-config.xml: &gt; &lt;infinispan&gt; &lt;cache-container&gt; &lt;serialization marshaller="org.infinispan.commons.marshall.JavaSerializationMarshaller"&gt; &lt;allow-list&gt; &lt;class&gt;example.EvictReloadTask&lt;/class&gt; &lt;/allow-list&gt; &lt;/serialization&gt; &lt;/cache-container&gt; &lt;/infinispan&gt;&lt;/pre&gt; &lt;p&gt;Then, we deploy the ConfigMap:&lt;/p&gt; &lt;pre&gt; $ oc apply -f cluster-config.yaml&lt;/pre&gt; &lt;h2&gt;Deploy Data Grid&lt;/h2&gt; &lt;p&gt;To deploy the Data Grid cluster in OpenShift we use the Data Grid operator.&lt;/p&gt; &lt;p&gt;Install the operator:&lt;/p&gt; &lt;pre&gt; $ oc apply -f infinispan-operator.yaml $ oc apply -f subscription.yaml&lt;/pre&gt; &lt;p&gt;In this example, we will use the following custom resource to let the operator create a Data Grid cluster:&lt;/p&gt; &lt;pre&gt; apiVersion: infinispan.org/v1 kind: Infinispan metadata: name: infinispan namespace: infinispan-demo spec: security: endpointEncryption: type: None clientCert: None expose: type: LoadBalancer dependencies: artifacts: - maven: 'org.postgresql:postgresql:42.3.1' - url: &gt;- https://github.com/torbjorndahlen/infinispan-evict-cache/raw/main/ServerTask/server/target/ServerTask.jar service: type: DataGrid replicas: 1 configMapName: cluster-config&lt;/pre&gt; &lt;p&gt;The Maven artifact refers to the JDBC driver for PostgreSQL. The URL artifact refers to the Git repository where the server task JAR file can be downloaded.&lt;/p&gt; &lt;p&gt;The Data Grid cluster is created with oc create:&lt;/p&gt; &lt;pre&gt; $ oc create -f infinispan-cr.yaml&lt;/pre&gt; &lt;p&gt;Use &lt;code&gt;oc get svc&lt;/code&gt; to find the URL to the Data Grid console:&lt;/p&gt; &lt;pre&gt; $ oc get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE example-infinispan-external LoadBalancer 172.30.5.74 my_host.example.com 11222:31797/TCP 6m39s&lt;/pre&gt; &lt;p&gt;In this example, we exposed Data Grid through a loadbalancer. The URL to the Data Grid contains the loadbalancer hostname and port. The Data Grid console can be accessed at &lt;code&gt;http://my_host.example.com:11222&lt;/code&gt;. The console username and password is stored in the &lt;code&gt;infinispan-generated-secret&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt; $ oc get secret infinispan-generated-secret -o jsonpath="{.data.identities\.yaml}" | base64 --decode credentials: - username: developer password: my_password roles: - admin&lt;/pre&gt; &lt;h2&gt;Create the cache&lt;/h2&gt; &lt;p&gt;You can use SQL stores with database tables that contain composite primary keys or composite values.&lt;/p&gt; &lt;p&gt;To use composite keys or values, you must provide Data Grid with protobuf schema that describe the data types. You must also add schema configuration to your SQL store and specify the message names for keys and values.&lt;/p&gt; &lt;p&gt;You can find more information on &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.4/html-single/cache_encoding_and_marshalling/index#doc-wrapper"&gt;cache encoding and marshaling&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Using the Data Grid console, create a cache using a protobuf schema and SQL cache store configuration, as shown in Figure 1:&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/skarmavbild_2023-04-20_kl._12.38.50.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/skarmavbild_2023-04-20_kl._12.38.50.png?itok=Xd9U3r3w" width="600" height="452" alt="A screenshot of the Data Grid console, creating a cache using a protobuf schema." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: Creating a cache using a protobuf schema in the Data Grid console.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Use the following configuration for the cache. In the configuration, &lt;code&gt;table-name&lt;/code&gt; refers to the &lt;code&gt;model&lt;/code&gt; table created when PostgreSQL was deployed. The &lt;code&gt;message-name&lt;/code&gt; and &lt;code&gt;package&lt;/code&gt; refers to the protobuf schema. Notice the user &lt;code&gt;infinispan&lt;/code&gt; that was previously created is used by the JDBC driver.&lt;/p&gt; &lt;pre&gt; { "distributed-cache": { "mode": "SYNC", "encoding": { "key": { "media-type": "application/x-protostream" }, "value": { "media-type": "application/x-protostream" } }, "persistence": { "table-jdbc-store": { "shared": true, "segmented": false, "dialect": "POSTGRES", "table-name": "model", "schema": { "message-name": "model_value", "package": "example" }, "connection-pool": { "connection-url": "jdbc:postgresql://postgresql-12-custom:5432/rpi-store", "driver": "org.postgresql.Driver", "username": "infinispan", "password": "secret" } } } } }&lt;/pre&gt; &lt;p&gt;When created, the cache will automatically load the &lt;code&gt;model&lt;/code&gt; table, as shown in Figure 2:&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/skarmavbild_2023-04-20_kl._12.52.01.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/skarmavbild_2023-04-20_kl._12.52.01.png?itok=0NTHk2Jh" width="600" height="489" alt="Shows the cache with entries from the model table." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: The cache is loaded with entries from the model table.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Run the server task&lt;/h2&gt; &lt;p&gt;In our example, the table from which Data Grid loaded the entries to the cache will be modified without notifying Data Grid. To update the cache with the modified entry, the server task will be called from a Hot Rod client.&lt;/p&gt; &lt;p&gt;First, we update a row in the model table:&lt;/p&gt; &lt;pre&gt; $ oc exec postgresql-12-custom-0 -- psql -U postgres -d rpi-store -c "update model set name = 'Raspberry Pi UPDATED' where id = 1;" $ oc exec postgresql-12-custom-0 -- psql -U infinispan -d rpi-store -c "select * from model;" id | name | model | soc | memory_mb | ethernet | release_year ----+-----------------------+-------+---------+-----------+----------+-------------- 2 | Raspberry Pi Zero | Zero | BCM2835 | 512 | f | 2015 3 | Raspberry Pi Zero | 2W | BCM2835 | 512 | f | 2021 1 | Raspberry Pi UPDATED | B | BCM2835 | 256 | t | 2012 (3 rows)&lt;/pre&gt; &lt;p&gt;Verify that the cache doesn't contain the updated entry by using the Infinispan CLI to lookup the key 1 in the cache:&lt;/p&gt; &lt;pre&gt; $ oc get pods NAME READY STATUS RESTARTS AGE infinispan-0 1/1 Running 0 86s $ oc rsh infinispan-0 sh-4.4$./bin/cli.sh [disconnected]&gt; connect Username: developer Password: my_password [infinispan-0-28040@infinispan//containers/default]&gt; cd caches [infinispan-0-28040@infinispan//containers/default/caches]&gt; cd rpi-store [infinispan-0-28040@infinispan//containers/default/caches/rpi-store]&gt; get 1 { "_type" : "example.model_value", "name" : "Raspberry Pi", "model" : "B", "soc" : "BCM2835", "memory_mb" : 256, "ethernet" : true, "release_year" : 2012 }&lt;/pre&gt; &lt;p&gt;The cache hasn't been notified about the modified row and still contains the name, &lt;strong&gt;Raspberry Pi&lt;/strong&gt;.&lt;/p&gt; &lt;p&gt;To load the modified entry into the cache, we run the client. The username and password are located under &lt;strong&gt;Secrets&lt;/strong&gt; in &lt;code&gt;infinispan-generated-secret&lt;/code&gt;.&lt;strong&gt; &lt;/strong&gt;The Data Grid server can be accessed from the infinispan loadbalancer hostname and port.&lt;/p&gt; &lt;pre&gt; $ git clone https://github.com/torbjorndahlen/infinispan-evict-cache.git $ cd infinispan-evict-cache/ServerTask/client $ mvn clean package $ mvn assembly:assembly -DdescriptorId=jar-with-dependencies $ java -cp target/ServerTaskClient-jar-with-dependencies.jar \ &gt; example.CacheServerTaskInvocation \ &gt; my_host.example.com 11222 \ &gt; developer my_password rpi-store&lt;/pre&gt; &lt;p&gt;Verify that the cache now contains the modified entries by using the Infinispan CLI to lookup the key 1 in the cache:&lt;/p&gt; &lt;pre&gt; $ oc rsh infinispan-0 sh-4.4$./bin/cli.sh [disconnected]&gt; connect Username: developer Password: my_password [infinispan-0-28040@infinispan//containers/default]&gt; cd caches [infinispan-0-28040@infinispan//containers/default/caches]&gt; cd rpi-store [infinispan-0-28040@infinispan//containers/default/caches/rpi-store]&gt; get 1 { "_type" : "example.model_value", "name" : "Raspberry Pi UPDATED", "model" : "B", "soc" : "BCM2835", "memory_mb" : 256, "ethernet" : true, "release_year" : 2012 }&lt;/pre&gt; &lt;p&gt;The updated value for &lt;strong&gt;key 1&lt;/strong&gt; has been loaded into the cache.&lt;/p&gt; &lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;In this article, we demonstrated how a cache using a PostgreSQL database as a cache store can be refreshed by using a server task invoked from a Hot Rod client when a table is being updated by other means than passing through Data Grid.&lt;/p&gt; &lt;p&gt;Thanks to Tristan Tarrant at Red Hat for providing advice and suggestions on the implementation of this tutorial.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/07/18/how-run-custom-server-task-red-hat-data-grid" title="How to run a custom server task in Red Hat Data Grid"&gt;How to run a custom server task in Red Hat Data Grid&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Torbjorn Dahlen</dc:creator><dc:date>2023-07-18T07:00:00Z</dc:date></entry><entry><title type="html">Keycloak 22.0.1 released</title><link rel="alternate" href="https://www.keycloak.org/2023/07/keycloak-2201-released" /><author><name /></author><id>https://www.keycloak.org/2023/07/keycloak-2201-released</id><updated>2023-07-18T00:00:00Z</updated><content type="html">To download the release go to . MIGRATION FROM 21.1 Before you upgrade remember to backup your database. If you are not on the previous release refer to for a complete list of migration changes. ALL RESOLVED ISSUES ENHANCEMENTS * Revisit Pod-Template in Keycloak CR keycloak operator * Support configurable custom Identity Providers keycloak * [REG 21-&gt;22] Error messages on kc build keycloak dist/quarkus BUGS * Accessibility/Clients List: Minor Issues keycloak admin/ui * `keycloakCRName` and `realm` are no longer marked as required in KeycloakRealmImport CRD keycloak operator * Version 22.0.0 not started in dev mode and build mode keycloak dist/quarkus * Migration for 22.0.0 is missing from the documentation keycloak docs * Broken links to quickstarts in documentation keycloak docs * Account V3 Missing translate Refresh keycloak account/ui * Keycloak is storing error events even if storing events is disabled keycloak storage * Fixing broken JSON translation files keycloak admin/ui UPGRADING Before you upgrade remember to backup your database and check the for anything that may have changed.</content><dc:creator /></entry><entry><title type="html">How to run standalone Jakarta Batch Jobs</title><link rel="alternate" href="https://www.mastertheboss.com/java-ee/batch-api/running-batch-jobs-in-j2se-applications/" /><author><name>F.Marchioni</name></author><id>https://www.mastertheboss.com/java-ee/batch-api/running-batch-jobs-in-j2se-applications/</id><updated>2023-07-14T10:16:48Z</updated><content type="html">Jakarta Batch, formerly known as Java Batch, is a specification that provides a standardized approach for implementing batch processing in Java applications. It offers a robust and scalable framework for executing large-scale, long-running, and data-intensive tasks. In this tutorial, we will explore the process of running Jakarta Batch Jobs as standalone Java applications, discussing the ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>A developer’s path to success with OpenShift and containers</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/07/13/developers-path-success-openshift-and-containers" /><author><name>Valentina Rodriguez Sosa</name></author><id>e01ea10c-6333-49c0-8681-48b4641c4ebe</id><updated>2023-07-13T07:00:00Z</updated><published>2023-07-13T07:00:00Z</published><summary type="html">&lt;p&gt;I am a developer new to containers, Kubernetes, or CI/CD. Where should I start?&lt;/p&gt; &lt;p&gt;This article provides five pathways including resources to succeed on your container journey.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Highlighted material:&lt;/strong&gt;&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;The following materials are free with no prerequisites.&lt;/li&gt; &lt;li aria-level="1"&gt;These materials are foundational for you to start working on your next project ASAP.&lt;/li&gt; &lt;li aria-level="1"&gt;Training materials will take up to five hours to complete.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;1. Start building your skills with containers and OpenShift&lt;/h2&gt; &lt;p&gt;To start with containers, understand what containers are and how CI/CD can automate the software development lifecycle. &lt;/p&gt; &lt;ul&gt;&lt;li aria-level="2"&gt;Documentation: &lt;a href="https://www.redhat.com/en/topics/containers#overview"&gt;Understanding containers &lt;/a&gt;&lt;/li&gt; &lt;li aria-level="2"&gt;Blog: &lt;a href="https://developers.redhat.com/blog/2020/09/03/the-present-and-future-of-ci-cd-with-gitops-on-red-hat-openshift#"&gt;The present and future of CI/CD with GitOps on Red Hat OpenShift &lt;/a&gt;&lt;/li&gt; &lt;li aria-level="2"&gt;Documentation: &lt;a href="https://www.redhat.com/en/topics/cloud-native-apps"&gt;Understanding Cloud Native Applications&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Start building your skills&lt;/h3&gt; &lt;p&gt;Gather hands-on experience with video tutorials and learning paths to practice the concepts learned from foundational to advanced on OpenShift.&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="2"&gt;Video tutorial: &lt;a href="https://developers.redhat.com/learn/openshift/foundations-openshift"&gt;Foundations of OpenShift&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="2"&gt;Video tutorial: &lt;a href="https://developers.redhat.com/learn/openshift"&gt;OpenShift and Kubernetes learning&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="2"&gt;Try the Developer Sandbox for Red Hat OpenShift: &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Start exploring in the Developer Sandbox for free&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Learn more about Kubernetes&lt;/h3&gt; &lt;p&gt;A deep dive on Kubernetes concepts from services to containers and pods: &lt;/p&gt; &lt;ul&gt;&lt;li&gt;Article: &lt;a href="https://developers.redhat.com/articles/2023/04/05/kubernetes-patterns-path-cloud-native"&gt;Kubernetes Patterns: The path to cloud native&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Our product documentation&lt;/h3&gt; &lt;p&gt;Discover all the features and capabilities of OpenShift from our product documentation.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://docs.openshift.com/container-platform/4.13/openshift_images/index.html"&gt;Overview of Images in Red Hat OpenShift &lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.openshift.com/container-platform/4.13/applications/index.html"&gt;Building Applications with Red Hat OpenShift &lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.openshift.com/container-platform/4.13/web_console/web-console-overview.html"&gt;OpenShift Web Console Overview&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;2. Modernize your applications&lt;/h2&gt; &lt;p&gt;Explore the practices to move your application to containers.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Video tutorial and additional materials: &lt;a href="https://developers.redhat.com/topics/microservices"&gt;Developing microservices on Kubernetes&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Documentation: &lt;a href="https://www.redhat.com/en/topics/application-modernization/what-is-dotnet-modernization#:~:text=The%20purpose%20of%20workload%20modernization,and%20integrating%20old%20with%20new."&gt;What is .NET application modernization?&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Article: &lt;a href="https://developers.redhat.com/articles/2023/05/15/how-use-new-openshift-quick-starts-deploy-jboss-eap"&gt;OpenShift QuickStarts to deploy JBossEAP&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Ready to practice?&lt;/h3&gt; &lt;p&gt;Practice the concepts learn with our Developer Sandbox for Red Hat OpenShift, tutorials, and hands-on labs.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Tutorials: &lt;a href="https://developers.redhat.com/topics"&gt;All Development topics with Red Hat Developer &lt;/a&gt;&lt;/li&gt; &lt;li&gt;Video tutorial: &lt;a href="https://www.redhat.com/en/services/training/do092-developing-cloud-native-applications-microservices-architectures"&gt;Developing cloud-native applications with microservices&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Hands-on lab: &lt;a href="https://developers.redhat.com/learn/openshift/develop-on-openshift"&gt;Developing on OpenShift&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Learn more about application development&lt;/h3&gt; &lt;p&gt;Learn about Red Hat Enterprise Linux capabilities to improve the developer experience and container applications development experience.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Article: &lt;a href="https://developers.redhat.com/articles/2023/05/10/how-new-rhel-92-improves-developer-experience#"&gt;How the new RHEL 9.2 improves the developer experience &lt;/a&gt;&lt;/li&gt; &lt;li&gt;Article: &lt;a href="https://developers.redhat.com/articles/2022/12/12/kubernetes-native-inner-loop-development-quarkus"&gt;Kubernetes-native inner loop development with Quarkus&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;3. Migrate at scale with OpenShift&lt;/h2&gt; &lt;p&gt;After migrating a couple of applications, you might wonder how we can replicate this process across an organization. Discover where to start with the modernization journey and how the developer experience can be improved.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Demo video: &lt;a href="https://www.youtube.com/watch?v=Pe0bFA4WawQ"&gt;Build, test, tune, and deploy your application with Red Hat OpenShift Dev Spaces&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Article: &lt;a href="https://developers.redhat.com/articles/2023/05/23/podman-desktop-now-generally-available"&gt;Podman Desktop 1.0: Local container development made easy &lt;/a&gt;&lt;/li&gt; &lt;li&gt;Documentation: &lt;a href="https://docs.openshift.com/container-platform/4.13/applications/odc-viewing-application-composition-using-topology-view.html"&gt;Viewing application composition using the Topology view &lt;/a&gt;&lt;/li&gt; &lt;li&gt;Documentation: &lt;a href="https://www.redhat.com/en/topics/application-modernization"&gt;Modernizing existing applications&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Ready to try?&lt;/h3&gt; &lt;p&gt;Start analyzing and assessing applications with MTA. Learn from our demo and product documentation.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://www.youtube.com/watch?v=u9N-T-uD_KU"&gt;Migration Toolkit For Applications&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Learn more about migration&lt;/h3&gt; &lt;p&gt;Plan your Java application modernization journey with our e-book and learn Podman's capabilities.&lt;/p&gt; &lt;ul&gt;&lt;li class="Indent1"&gt;Article: &lt;a href="https://developers.redhat.com/articles/2022/05/02/podman-basics-resources-beginners-and-experts#"&gt;Podman basics &lt;/a&gt;&lt;/li&gt; &lt;li class="Indent1"&gt;E-book: &lt;a href="https://www.redhat.com/en/engage/java-application-modernization-20220926"&gt;A practical guide to kick-start your own initiative&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;4. Automate to accelerate your software development lifecycle&lt;/h2&gt; &lt;p&gt;Automate software development process adopting GitOps approach and secure with DevSecOps.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Article: &lt;a href="https://developers.redhat.com/articles/2022/09/07/how-set-your-gitops-directory-structure"&gt;How to set up your GitOps directory structure&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Article: &lt;a href="https://developers.redhat.com/articles/2022/07/20/git-workflows-best-practices-gitops-deployments"&gt;Git best practices: Workflows for GitOps deployments&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Documentation: &lt;a href="https://www.redhat.com/en/topics/devops/what-is-devsecops"&gt;What's DevSecOps &lt;/a&gt;&lt;/li&gt; &lt;li&gt;Documentation and demos: &lt;a href="https://developers.redhat.com/topics/devsecops"&gt;DevSecOps: Automating security in the development lifecycle&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Ready to try automation?&lt;/h3&gt; &lt;p&gt;Learn from these free hands-on labs how to bring automation with CI/CD and GitOps practices by using Helm, OpenShift Pipelines, Jenkins, Ansible Automation Platform, and OpenShift GitOps.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/learn/openshift/develop-gitops"&gt;Develop with GitOps &lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/courses/gitops/getting-started-openshift-pipelines"&gt;Getting Started with OpenShift Pipelines&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/courses/cicd-ansible-automation-platform-and-jenkins-openshift"&gt;CI/CD with the Ansible Automation Platform and Jenkins on OpenShift &lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/courses/gitops/working-helm"&gt;Working with Helm&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Learn more about DevOps&lt;/h3&gt; &lt;p&gt;These e-books will help you start with best practices and practical guides to transform into a DevOps culture.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/e-books/path-gitops"&gt;The Path to GitOps &lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.redhat.com/en/engage/devops-culture-practice-openshift-ebooks"&gt;DevOps Culture and Practice with OpenShift&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Our product documentation&lt;/h3&gt; &lt;p&gt;Review our product documentation to learn about features and much more.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://docs.openshift.com/container-platform/4.13/cicd/index.html"&gt;OpenShift CI/CD &lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.openshift.com/container-platform/4.13/applications/working_with_helm_charts/understanding-helm.html"&gt;Understanding Helm &lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.openshift.com/container-platform/4.13/cicd/gitops/understanding-openshift-gitops.html"&gt;Understanding OpenShift GitOps&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;5. How to innovate with OpenShift&lt;/h2&gt; &lt;p&gt;Learn about key OpenShift capabilities to bring innovation to applications from serverless architectures, interconnecting services in diverse platforms, and securing and observing microservices with OpenShift Service Mesh.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Documentation: &lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift/serverless"&gt;What's Red Hat OpenShift &lt;/a&gt;&lt;/li&gt; &lt;li&gt;Documentation: &lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift/serverless"&gt;Serverless&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Documentation: &lt;a href="https://developers.redhat.com/products/service-interconnect/overview"&gt;Interconnect applications and microservices across the open hybrid cloud&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Tutorials, books, videos and more: &lt;a href="https://developers.redhat.com/topics/serverless-architecture#assembly-field-sections-38375"&gt;Build serverless architectures for Kubernetes with Knative &lt;/a&gt;&lt;/li&gt; &lt;li&gt;Documentation: &lt;a href="https://www.redhat.com/en/technologies/cloud-computing/openshift/what-is-openshift-service-mesh"&gt;What's Red Hat OpenShift Service Mesh&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Ready to try OpenShift components?&lt;/h3&gt; &lt;p&gt;Gather hands-on experience with our free labs and follow tutorials and demos at your own pace.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Hands-on lab: &lt;a href="https://developers.redhat.com/courses/getting-started-openshift-serverless"&gt;Getting Started with OpenShift Serverless &lt;/a&gt;&lt;/li&gt; &lt;li&gt;Demo: &lt;a href="https://www.youtube.com/watch?v=YoGR5zZGG9k"&gt;OpenShift Service Mesh&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Learn more about OpenShift Service Mesh&lt;/h3&gt; &lt;p&gt;This e-book provides guidance on governance, design practices, and configuring Red Hat OpenShift Service Mesh for production use and performing day-2 operations. &lt;/p&gt; &lt;ul&gt;&lt;li&gt;E-book: &lt;a href="https://www.redhat.com/en/resources/getting-started-with-openshift-service-mesh-ebook"&gt;Getting Started with Red Hat OpenShift Service Mesh&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Find more resources in our product documentation&lt;/h3&gt; &lt;p&gt;Learn about product capabilities, features, and much more from our product documentation.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://docs.openshift.com/container-platform/4.13/service_mesh/v2x/ossm-about.html"&gt;OpenShift Service Mesh &lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.openshift.com/container-platform/4.13/serverless/about/about-serverless.html"&gt;OpenShift Serverless &lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://docs.openshift.com/container-platform/4.13/distr_tracing/distributed-tracing-release-notes.html"&gt;Distributed Tracing&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/07/13/developers-path-success-openshift-and-containers" title="A developer’s path to success with OpenShift and containers"&gt;A developer’s path to success with OpenShift and containers&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Valentina Rodriguez Sosa</dc:creator><dc:date>2023-07-13T07:00:00Z</dc:date></entry><entry><title>Quarkus Newsletter #34 - July</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-newsletter-34/&#xA;            " /><author><name>James Cobb (https://twitter.com/insectengine)</name></author><id>https://quarkus.io/blog/quarkus-newsletter-34/</id><updated>2023-07-13T00:00:00Z</updated><published>2023-07-13T00:00:00Z</published><summary type="html">Read "Quarkus 3.2.0.Final released - New security features, @QuarkusComponentTest" by Guillaume Smet" to learn about major changes like; various new security features, the ability to test CDI components with @QuarkusComponentTest and new build time analytics. Kevin Dubois' article "Managing Java containers with Quarkus and Podman Desktop" shows how to build...</summary><dc:creator>James Cobb (https://twitter.com/insectengine)</dc:creator><dc:date>2023-07-13T00:00:00Z</dc:date></entry><entry><title>How to create an instance on GCP using the Ansible CLI</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/07/12/how-create-instance-gcp-using-ansible-cli" /><author><name>Deepankar Jain</name></author><id>db5556aa-b189-409a-9e92-a77ccdafba55</id><updated>2023-07-12T07:00:00Z</updated><published>2023-07-12T07:00:00Z</published><summary type="html">&lt;p&gt;This series covers the end-to-end process of creating an instance on Google Cloud Platform (GCP) using Red Hat Ansible Automation Platform. This 3-part series includes:&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Part 1: How to create an instance on GCP using Ansible CLI&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Part 2: &lt;a href="https://developers.redhat.com/articles/2023/07/12/how-create-gcp-instance-using-ansible-automation"&gt;How to create a GCP instance using Ansible Automation&lt;/a&gt;&lt;/p&gt; &lt;p&gt;Part 3: &lt;a href="https://developers.redhat.com/articles/2023/07/12/how-create-gcp-instance-workflow-and-ansible"&gt;How to create a GCP instance via workflow and Ansible&lt;/a&gt;&lt;/p&gt; &lt;p&gt;By the end of this article, you will have a clear understanding of how to use the Ansible Automation Platform CLI to automate the creation of GCP instances, which will save you time and reduce the risk of manual errors. Let's get started!&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;a href="https://developers.redhat.com/products/ansible/getting-started"&gt;Ansible&lt;/a&gt; installed on your system.&lt;/li&gt; &lt;li aria-level="1"&gt;An active GCP Account with sufficient permissions.&lt;/li&gt; &lt;li aria-level="1"&gt;Ansible &lt;a href="https://galaxy.ansible.com/google/cloud"&gt;google cloud collection&lt;/a&gt; installed on your system.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;We will start by setting up the necessary credentials and roles for our Ansible playbook to access the GCP API. Then we will create a disk, a network, a security group, and an IP address before finally launching the instance.&lt;/p&gt; &lt;h2&gt;How to use Ansible CLI&lt;/h2&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Create a &lt;a href="https://support.google.com/a/answer/7378726?hl=en"&gt;service account&lt;/a&gt; in GCP.&lt;/li&gt; &lt;li aria-level="1"&gt;Generate the &lt;a href="https://developers.google.com/workspace/guides/create-credentials#create_credentials_for_a_service_account"&gt;credentials&lt;/a&gt; for the service account.&lt;/li&gt; &lt;li aria-level="1"&gt;You should now have a credential.json&lt;strong&gt; &lt;/strong&gt;file&lt;strong&gt; &lt;/strong&gt;that you can use to access your GCP account and launch an instance.&lt;/li&gt; &lt;li aria-level="1"&gt;Open any editor and copy the following yml into it.&lt;/li&gt; &lt;/ul&gt;&lt;pre&gt; &lt;code class="language-yaml"&gt;--- - name: Create instance in GCP   hosts: localhost   gather_facts: false   vars:     service_account_file: "&lt;path to service account file&gt;"     project: "&lt;SOMETHING&gt;"     network_name: "test-ansible-network"     subnet_name: "test-ansible-subnet"     ip_name: "test-ansible-ip"     disk_name: "test-ansible-disk"     machine_name: "test-ansible"     region: "asia-south2"     zone: "asia-south2-a"     source_image: "projects/ubuntu-os-cloud/global/images/family/ubuntu-1804-lts"     subnet_cidr: "10.0.1.0/24"     disk_size: 10     machine_type: "f1-micro"   tasks:     - name: Create a disk       google.cloud.gcp_compute_disk:         name: "{{ disk_name }}"         size_gb: "{{ disk_size }}"         source_image: "{{ source_image }}"         zone: "{{ zone }}"         project: "{{ project }}"         auth_kind: serviceaccount         service_account_file: "{{ service_account_file }}"         state: present       register: disk          - name: Create a Network in GCP       google.cloud.gcp_compute_network:         auth_kind: serviceaccount         project: "{{ project }}"         service_account_file: "{{ service_account_file }}"         name: "{{ network_name }}"         auto_create_subnetworks: false         state: present       register: network     - name: Create a Subnet in the Network       google.cloud.gcp_compute_subnetwork:         auth_kind: serviceaccount         project: "{{ project }}"         service_account_file: "{{ service_account_file }}"         name: "{{ subnet_name }}"         region: "{{ region }}"         ip_cidr_range: "{{ subnet_cidr }}"         network: "{{ network }}"         state: present       register: subnet     - name: Reserve a static IP Address       google.cloud.gcp_compute_address:         auth_kind: serviceaccount         project: "{{ project }}"         service_account_file: "{{ service_account_file }}"         name: "{{ ip_name }}"         region: "{{ region }}"         state: present       register: address              - name: Create an Instance        google.cloud.gcp_compute_instance:         auth_kind: serviceaccount         project: "{{ project }}"         service_account_file: "{{ service_account_file }}"         state: present         name: "{{ machine_name }}"         machine_type: "{{ machine_type }}"         zone: "{{ zone }}"         disks:           - auto_delete: true             boot: true             source: "{{ disk }}"         network_interfaces:           - network: "{{ network }}"             subnetwork: "{{ subnet }}"             access_configs:               - name: External NAT                 type: ONE_TO_ONE_NAT                 nat_ip: "{{ address }}" &lt;/code&gt;&lt;/pre&gt; &lt;ul&gt;&lt;li&gt;Save and close the file.&lt;/li&gt; &lt;li&gt;Then open the terminal in the directory where the file is located.&lt;/li&gt; &lt;li aria-level="1"&gt;Now run the following command: &lt;pre&gt; &lt;code class="language-bash"&gt;ansible-playbook &lt;filename&gt;.yml&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The output:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;PLAY [Create instance in GCP] ************************************************************************************************************************************************************ TASK [Gathering Facts] ******************************************************************************************************************************************************************************************** ok: [localhost] TASK [Create a disk] ************************************************************************************************************************************************************************************ changed: [localhost] TASK [Create a Network in GCP] *********************************************************************************************************************************************************************************** changed: [localhost] TASK [Create a Subnet in the Network] ******************************************************************************************************************************************************************************************** changed: [localhost] TASK [Reserve a static IP Address] ********************************************************************************************************************************************************************************* changed: [localhost] TASK [Create an Instance] ******************************************************************************************************************************************* changed: [localhost] PLAY RECAP ******************************************************************************************************************************************************************************************************** localhost : ok=6 changed=5 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The GCP instance is shown in Figure 1.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_from_2023-04-27_10-54-25.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_from_2023-04-27_10-54-25.png?itok=g2s4Y660" width="600" height="169" alt="Creating a GCP instance." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: Creating a GCP instance.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;By following the step-by-step guide, you should now have a good understanding of how to use Ansible to automate the creation of a virtual machine. To learn more about Ansible and access additional resources and guides, including diverse examples and use cases, we recommend visiting &lt;a href="https://developers.redhat.com/learn/ansible"&gt;Red Hat Ansible Automation Platform&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;What’s next?&lt;/h2&gt; &lt;p&gt;In our &lt;a href="https://developers.redhat.com/articles/2023/07/12/how-create-gcp-instance-using-ansible-automation"&gt;next article&lt;/a&gt;, we will explore how &lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;Ansible Automation Platform&lt;/a&gt; further eases the process of creating virtual machines by enabling you to define infrastructure as code, track infrastructure changes, and enforce compliance policies. If you're interested in exploring how to use &lt;a href="https://developers.redhat.com/learn/ansible"&gt;Ansible Automation Platform&lt;/a&gt; on Azure, you can also access the &lt;a href="https://developers.redhat.com/content-gateway/link/3872066"&gt;lab&lt;/a&gt;. This lab allows you to try Ansible Automation Platform on Azure and learn how it can be used to automate infrastructure deployment.&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/products/ansible/getting-started"&gt;Get started&lt;/a&gt; with Ansible Automation Platform by exploring interactive hands-on labs. &lt;a href="https://developers.redhat.com/products/ansible/download"&gt;Download Ansible Automation Platform&lt;/a&gt; at no cost and begin your automation journey. You can refer to &lt;a href="https://developers.redhat.com/e-books/choosing-automation-tool"&gt;An IT executive's guide to automation&lt;/a&gt; e-book for a better understanding of the Ansible Automation Platform.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/07/12/how-create-instance-gcp-using-ansible-cli" title="How to create an instance on GCP using the Ansible CLI"&gt;How to create an instance on GCP using the Ansible CLI&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Deepankar Jain</dc:creator><dc:date>2023-07-12T07:00:00Z</dc:date></entry><entry><title>On the Road to CDI Compatibility</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/on-the-road-to-cdi-compatibility/&#xA;            " /><author><name>Ladislav Thon (https://twitter.com/_Ladicek)</name></author><id>https://quarkus.io/blog/on-the-road-to-cdi-compatibility/</id><updated>2023-07-12T00:00:00Z</updated><published>2023-07-12T00:00:00Z</published><summary type="html">Ever since the very first days of Quarkus, the days that are now covered by the blissful fog of oblivion and the survivors only talk about them after a few pints of beer, dependency injection container was an integral part of the envisioned framework. And not just any dependency injection...</summary><dc:creator>Ladislav Thon (https://twitter.com/_Ladicek)</dc:creator><dc:date>2023-07-12T00:00:00Z</dc:date></entry><entry><title type="html">Groupby &amp;#8211; a new way to accumulate facts in DRL</title><link rel="alternate" href="https://blog.kie.org/2023/07/groupby-a-new-way-to-accumulate-facts-in-drl.html" /><author><name>Christopher Chianelli</name></author><id>https://blog.kie.org/2023/07/groupby-a-new-way-to-accumulate-facts-in-drl.html</id><updated>2023-07-11T09:00:00Z</updated><content type="html">Have you ever wanted to accumulate facts that share a particular property (for instance, the count of people in each department) in DRL? With the addition of groupby to DRL, it is easier to write rules that divide facts into groups where each group is accumulated separately. In this post, we’ll explain what groupby is, how to use it, and give examples of groupby usage. WHAT IS GROUPBY? Groupby is a new syntax construct introduced in 8.41.0.Final for dividing facts into groups, allowing each group to be accumulated separately. It has the following syntax: groupby(source-pattern; grouping-key; accumulators [; constraints]) Where: * source-pattern: Pattern used to gather facts that will be grouped and accumulated. For instance, using $a: /applicants[age &lt; 21] as the source-pattern would group and accumulate all Applicants under the age of 21 (binding the applicant to $a, which can be used in the grouping-key and accumulators). The examples in this article use the OOPath syntax for source-pattern, but the traditional syntax is also supported. For more details, see . * grouping-key: A function used to divide the source-pattern into separate groups. Facts from source-pattern for which grouping-key returns the same value are grouped together. The key can optionally be bound to a variable, allowing it to be used outside the groupby. For example, using $country: $a.country as the grouping key would group applicants by country, binding the country for the group to the $country variable. * accumulators: One or more accumulate functions used to accumulate the facts in each group. Each accumulate function can optionally be bound to a variable, allowing it to be used outside the groupby. For instance, $count: count() would count the number of applicants younger than 21 from each country. * constraints: Optional constraints on the group key and accumulation result. The rest of the rule will only execute for the given group key if all constraints return true. For instance, $count &gt;= 100 would only continue execution of the rule for a given $country if that country have at least 100 applicants younger than 21. Combining the above example together, we get: groupby( $a: /applicants[age &lt; 21]; $country: $a.country; $count: count(); $count &gt;= 100 ) HOW TO USE GROUPBY You can use groupby by upgrading Drools to 8.41.0.Final or later. Groupby can then be used in your DRL files like any other language construct (such as forall and accumulate). EXAMPLE GROUPBY USAGE Groupby is a good choice whenever you need to accumulate results for separate groups. Some examples of rules that can be implemented with it include: * Get departments over budget rule "Departments over budget" groupby($order: /order; $department: $order.department; $totalCost: sum($order.cost); $totalCost &gt; $department.budget ) then // ... end * Find days which are understaffed rule "Understaffed Days" groupby($shift: /shifts[ employee != null ]; $date: $shift.date; $assignedCount: count(); $assignedCount &lt; $date.minimumAssignedShifts ) then // ... end * Get the highest bid for a product rule "Highest bid for a product" groupby($bid: /bids; $product: $bid.product; $highestBid: max($bid) ) then // ... end CONCLUSION Groupby is a new language feature introduced in 8.41.0.Final that allows for simpler grouping of objects by a key function. You can use it by upgrading Drools to 8.41.0.Final or later. Groupby is useful for implementing rules that accumulate facts that share a given property, such as getting the total cost by department or getting the highest bidder by product. The post appeared first on .</content><dc:creator>Christopher Chianelli</dc:creator></entry><entry><title>How to deploy a MSSQL database using Ansible Vault</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/07/11/how-deploy-mssql-database-using-ansible-vault" /><author><name>Deepankar Jain</name></author><id>3d92a4ae-f9d6-4a0d-8f2b-0f9fe1975694</id><updated>2023-07-11T07:00:00Z</updated><published>2023-07-11T07:00:00Z</published><summary type="html">&lt;p&gt;Deploying and configuring a database can be a challenging task, especially when sensitive data such as passwords and API keys are involved. However, by using &lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;Red Hat Ansible Automation Platform&lt;/a&gt; and &lt;a href="https://docs.ansible.com/ansible/2.8/user_guide/vault.html#:~:text=Ansible%20Vault%20is%20a%20feature,or%20placed%20in%20source%20control."&gt;Ansible Vault&lt;/a&gt;, we can streamline the process and ensure that our data is secure. In this article, we will explore how to configure and deploy a Microsoft SQL database using Ansible Vault, an Ansible Automation Platform feature.&lt;/p&gt; &lt;p&gt;We will walk you through the process, step-by-step, so that you can easily follow along and try it for yourself. By the end of this article, you will have a solid understanding of how to use Ansible Vault to deploy a secure database, making your deployment process more efficient and secure.&lt;/p&gt; &lt;p&gt;Before you begin, make sure to &lt;a href="https://developers.redhat.com/products/ansible/download"&gt;download&lt;/a&gt; and install Ansible Automation Platform, available at no cost. For more information about Ansible Automation Platform installation, please refer to our previous article, &lt;a href="https://developers.redhat.com/articles/2023/01/01/how-install-red-hat-ansible-automation-platform-rhel-9#"&gt;How to install Red Hat Ansible Automation Platform on RHEL 9&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Enhancing database security with Ansible Automation Platform&lt;/h2&gt; &lt;p&gt;&lt;a href="https://docs.ansible.com/ansible/2.8/user_guide/vault.html#:~:text=Ansible%20Vault%20is%20a%20feature,or%20placed%20in%20source%20control."&gt;Ansible Vault&lt;/a&gt; is a powerful tool for keeping sensitive information, such as passwords, hidden from prying eyes. You can easily encrypt sensitive data and keep it separate from your code. This provides an added layer of security and ensures that only authorized personnel can access sensitive information. With this, you can manage passwords and other sensitive data in a secure manner, without having to modify the code itself. In this demonstration, we'll be working with a &lt;a href="https://github.com/redhat-developer-demos/MicrosoftSQL-AAP-on-RHEL"&gt;repository&lt;/a&gt; that demonstrates how to use Ansible Automation Platform to deploy and manage MSSQL databases. To protect the MSSQL password in our playbook, we made some modifications that improved our security and simplified our playbook management.&lt;/p&gt; &lt;p&gt;First, we removed the mssql_password key from our playbook, as this was a potential security vulnerability.&lt;/p&gt; &lt;p&gt;The original playbook:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;vars: mssql_password: "123@Redhat" mssql_edition: Evaluation ... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The modified playbook:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;vars: mssql_edition: Evaluation ... &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then, we created a new YAML file called mssql_password.yml to store the password separately from the playbook code. By doing this, we were able to keep the password secure and prevent it from being exposed in our code.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;mssql_password.yml&lt;/strong&gt;&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;vars: mssql_password: "123@Redhat" &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then encrypt the mssql_password.yml using ansible vault:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;ansible-vault encrypt mssql_password.yml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We will be prompted to enter a password to encrypt the file. Modify the playbook to include mssq_password as a vars file.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- - hosts: dev vars_files: sql_password.yml vars: …… &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We’re now ready to run this playbook on Ansible Automation Platform.&lt;/p&gt; &lt;h3&gt;Step 1: Set up the automation execution environment&lt;/h3&gt; &lt;p&gt;Automation execution environments provide a defined, consistent, and portable environment for executing automation jobs. Unlike legacy virtual environments, automation execution environments are Linux container images that make it possible to incorporate system-level dependencies and collection-based content. Each automation execution environment allows you to have a customized image to run jobs, and each of them contains only what you need when running the job.&lt;/p&gt; &lt;p&gt;There are dependencies for the automation execution environment, such as Python 3 and Podman. Make sure these tools are installed. We have provided instructions for installing and using Podman in this &lt;a href="https://developers.redhat.com/videos/youtube/bJDI_QuXeCE"&gt;video&lt;/a&gt;. Before you can complete any of the following tasks, you must create a registry &lt;a href="https://access.redhat.com/terms-based-registry/"&gt;service account&lt;/a&gt;. To log in to SA, you'll need to use a container runtime such as Podman or Docker. &lt;a href="https://developers.redhat.com/videos/youtube/bJDI_QuXeCE"&gt;Podman&lt;/a&gt; is a powerful and secure open source tool that can be used as an alternative to Docker, with the added benefits of not requiring a daemon to run containers and having a more lightweight footprint. If you don't have Podman &lt;a href="https://podman.io/getting-started/installation"&gt;installed&lt;/a&gt;, you can use Docker instead, but we recommend using Podman for a more efficient &lt;a href="https://developers.redhat.com/articles/podman-next-generation-linux-container-tools"&gt;container experience&lt;/a&gt;. To log in, open up your terminal and type the following commands:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman login registry.redhat.io Username: {REGISTRY-SERVICE-ACCOUNT-USERNAME} Password: {REGISTRY-SERVICE-ACCOUNT-PASSWORD}   Login Succeeded!&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once we are successfully logged in, we need to create a container image by using a container file containing the following context:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;FROM registry.redhat.io/ansible-automation-platform-22/ee-29-rhel8:latest RUN ansible-galaxy collection install microsoft.sql&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To build an image using &lt;code&gt;podman&lt;/code&gt;.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman build -t &lt;image-name&gt;.&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The image should be pushed into the container image registry. Log in to the private container image registry using the command &lt;code&gt;podman login&lt;/code&gt; before pushing.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman push &lt;image-name&gt;&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Add the image name in the automation execution environment, as shown in Figure 1.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_2023-04-13_003136.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_2023-04-13_003136.png?itok=mATj6vlV" width="600" height="292" alt="A screenshot of the execution environment page." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: The execution environment page.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Step 2: Set up the inventory&lt;/h3&gt; &lt;p&gt;An inventory is a collection of hosts against which jobs may be launched. To create inventory in Ansible Automation Platform, follow these steps and refer to Figure 2:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Select the inventory from the left menu.&lt;/li&gt; &lt;li aria-level="1"&gt;Click on &lt;strong&gt;add&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Select &lt;strong&gt;add inventory&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Give a name to the inventory and save it.&lt;/li&gt; &lt;li aria-level="1"&gt;Select the groups from inventories and click &lt;strong&gt;add group&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Give a name to the group and save it.&lt;/li&gt; &lt;li aria-level="1"&gt;Next, click on Hosts and click &lt;strong&gt;add a new host&lt;/strong&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Give the targeted server IP and save it.&lt;/li&gt; &lt;/ol&gt;&lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig-2-mssql.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig-2-mssql.png?itok=Tk-AnMXs" width="600" height="176" alt="A screenshot of the inventory page in Ansible." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: Creating inventory on the Inventory page.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Step 3: Set up the credentials&lt;/h3&gt; &lt;p&gt;To connect with the target server and decrypting the mssql_password.yml, we need credentials such as username, password, or ssh key of the target machine and password of the ansible vault. By using credentials, we can pass the required credentials during the playbook execution.&lt;/p&gt; &lt;p&gt;Follow these steps and refer to Figure 3 to set up the ssh machine credentials.&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Select the credentials from the left menu.&lt;/li&gt; &lt;li aria-level="1"&gt;Click on new credentials and select Machine credentials type.&lt;/li&gt; &lt;li aria-level="1"&gt;Add your username, password, or ssh key in the corresponding fields.&lt;/li&gt; &lt;li aria-level="1"&gt;(Optional) You'll be needed to run privileged commands on the remote machine, enter the sudo password in the Privilege Escalation section. This will allow Ansible controller to escalate privileges when necessary&lt;/li&gt; &lt;/ol&gt;&lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig-3-mssql-blog.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig-3-mssql-blog.png?itok=t-2j_aDL" width="600" height="219" alt="A screenshot of the machine credential page." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: The machine credentials page.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Set up the Ansible Vault credentials (Figure 4):&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Click on new credentials and select vault type.&lt;/li&gt; &lt;li aria-level="1"&gt;Add a name to the credential and the vault password as the password you used for encrypting your mssql_password.yml (in our case, 12345678).&lt;/li&gt; &lt;/ol&gt;&lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig-4-mssql.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig-4-mssql.png?itok=ONS0DWCO" width="600" height="190" alt="A screenshot of the vault credentials page." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 4: The vault credentials page.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Step 4: Configure a project&lt;/h3&gt; &lt;p&gt;A project is a logical collection of Ansible Playbooks represented in the controller. You can manage playbooks and playbook directories on your controller server either manually or by using a source code management (SCM) system, such as Git, Subversion, or Mercurial supported by the controller.&lt;/p&gt; &lt;p&gt;Follow these steps to create a project and refer to Figure 5:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Create a new project for our git repository from the left menu.&lt;/li&gt; &lt;li aria-level="1"&gt;Click on the + icon in the right corner.&lt;/li&gt; &lt;li aria-level="1"&gt;Give the project a name.&lt;/li&gt; &lt;li aria-level="1"&gt;Select your organization (or choose &lt;strong&gt;Default&lt;/strong&gt;).&lt;/li&gt; &lt;li aria-level="1"&gt;Select the SCM TYPE (GIT in our case).&lt;/li&gt; &lt;li aria-level="1"&gt;Add RESOURCE DETAILS&lt;/li&gt; &lt;li aria-level="1"&gt;SCM &lt;a href="https://github.com/decipher07/Integrate-Ansible-Vault-Controller"&gt;URL&lt;/a&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;SCM BRANCH(main)&lt;/li&gt; &lt;li aria-level="1"&gt;SCM CREDENTIAL&lt;/li&gt; &lt;li aria-level="1"&gt;Save it.&lt;/li&gt; &lt;/ol&gt;&lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_2023-04-13_003744.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_2023-04-13_003744.png?itok=XyfCsqWY" width="600" height="278" alt="Figure 4: The Templates page." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 5: The Templates page.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Step 5: Configure the templates&lt;/h3&gt; &lt;p&gt;&lt;a href="https://docs.ansible.com/automation-controller/latest/html/userguide/glossary.html#term-Job-Template"&gt;Templates&lt;/a&gt; define and set parameters for running jobs. A template is more like a blueprint where all of the dependencies are defined, such as inventory, projects, credentials, etc.&lt;/p&gt; &lt;p&gt;Follow these steps to create a template to execute the job for us (Figure 5):&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;From the left menu, select templates and create a new template.&lt;/li&gt; &lt;li aria-level="1"&gt;Click on the &lt;strong&gt;+&lt;/strong&gt; icon from the right corner and select the Job template.&lt;/li&gt; &lt;li aria-level="1"&gt;Give the template a name.&lt;/li&gt; &lt;li aria-level="1"&gt;Select the project and playbook you want to run in the template.&lt;/li&gt; &lt;li aria-level="1"&gt;Select &lt;a href="https://github.com/redhat-developer-demos/MicrosoftSQL-AAP-on-RHEL/blob/main/MicrosoftSQL-with-ansible-vault/microsoft_sql_playbook.yml"&gt;MicrosoftSQL-with-ansible-vault/microsoft_sql_playbook.yml&lt;/a&gt; playbook.&lt;/li&gt; &lt;li aria-level="1"&gt;Select the execution environment which you created previously and make sure to check the &lt;strong&gt;Privilege Escalation &lt;/strong&gt;option if you have that enabled with your ssh credentials.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;If you haven't set up the privilege escalation password during the SSH credential, you can use still run the playbook by setting a variable called &lt;strong&gt;ansible_sudo_pass&lt;/strong&gt; with the value as the password to your VM.&lt;/p&gt; &lt;ol start="7"&gt;&lt;li aria-level="1"&gt;Launch It (Figure 6).&lt;/li&gt; &lt;/ol&gt;&lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/fig-5-mssql-blog.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/fig-5-mssql-blog.png?itok=WgY97UEO" width="600" height="282" alt="A successful installation of Microsoft SQL server." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 6: A successful installation of Microsoft SQL server after launching.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;Using Ansible Automation Platform and Ansible Vault can greatly enhance the security of your database deployments by keeping sensitive information like passwords separate from the code and protecting it from unauthorized access. To explore more of what Ansible Automation Platform has to offer, visit the &lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;official website&lt;/a&gt; to &lt;a href="https://developers.redhat.com/products/ansible/download"&gt;download&lt;/a&gt; and &lt;a href="https://developers.redhat.com/products/ansible/getting-started"&gt;get started&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Additionally, these e-books can help you explore the capabilities of Ansible Automation Platform:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/e-books/automation-at-the-edge"&gt;Automation at the Edge&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/e-books/choosing-automation-tool"&gt;Choosing an Automation Tool&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/e-books/it-executives-guide-automation"&gt;An IT Executive's Guide to Automation&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;A cheat sheet is also available for &lt;a href="https://developers.redhat.com/cheat-sheets/wifi-automation-ansible-and-sd-wan-meraki-cheat-sheet"&gt;WiFi automation with Ansible and SD&lt;/a&gt;, providing a quick reference for network automation tasks.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/07/11/how-deploy-mssql-database-using-ansible-vault" title="How to deploy a MSSQL database using Ansible Vault"&gt;How to deploy a MSSQL database using Ansible Vault&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Deepankar Jain</dc:creator><dc:date>2023-07-11T07:00:00Z</dc:date></entry><entry><title type="html">Keycloak 22.0.0 released</title><link rel="alternate" href="https://www.keycloak.org/2023/07/keycloak-2200-released" /><author><name /></author><id>https://www.keycloak.org/2023/07/keycloak-2200-released</id><updated>2023-07-11T00:00:00Z</updated><content type="html">To download the release go to . MIGRATION FROM 21.1 Before you upgrade remember to backup your database. If you are not on the previous release refer to for a complete list of migration changes. ALL RESOLVED ISSUES ENHANCEMENTS * Revisit Pod-Template in Keycloak CR keycloak operator * Support configurable custom Identity Providers keycloak * [REG 21-&gt;22] Error messages on kc build keycloak dist/quarkus BUGS * Accessibility/Clients List: Minor Issues keycloak admin/ui * `keycloakCRName` and `realm` are no longer marked as required in KeycloakRealmImport CRD keycloak operator * Version 22.0.0 not started in dev mode and build mode keycloak dist/quarkus * Migration for 22.0.0 is missing from the documentation keycloak docs * Broken links to quickstarts in documentation keycloak docs * Account V3 Missing translate Refresh keycloak account/ui * Keycloak is storing error events even if storing events is disabled keycloak storage * Fixing broken JSON translation files keycloak admin/ui UPGRADING Before you upgrade remember to backup your database and check the for anything that may have changed.</content><dc:creator /></entry></feed>
